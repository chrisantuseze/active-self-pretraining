# distributed training
nodes: 1
gpus: 1                                       # I recommend always assigning 1 GPU to 1 node
dataset_dir: "./datasets"                     # path to dataset
backbone: "resnet50"
projection_dim: 128                           # "[...] to project the representation to a 128-dimensional latent space"

# loss options
weight_decay: 1.0e-6                          # "optimized using LARS [...] and weight decay of 1.0eâˆ’4" try changing to 1.0e-6 to see the performance
temperature: 0.1                              # see appendix B.7.: Optimal temperature under different batch sizes

# reload options
model_checkpoint_path: "save/checkpoints"     # directory where all checkpoints would be saved
model_misc_path: "save/misc"                  # directory where all misc files would be saved
epoch_num: 50                                 # use to determine the base checkpoint to be used for the AL cycle
reload: False                                 # indicates whether to start the training from the checkpoint or not

# pretrain options

method: 3                                     # 0 for SimCLR, 1 for DCL, 2 for MYOW, 3 for SwAV, 4 for SUPERVISED, 

################################ SIMCLR #######################################
simclr_batch_size: 512
simclr_epochs: 500
simclr_temperature: 0.5
simclr_optimizer: "SimCLR"
simclr_base_lr: 1.0e-3

################################ DCL #######################################
dcl_batch_size: 64                        # They experimented with 32 - 512, but stuck with 256 eventually for simclr, dcl, and dclw
dcl_base_epochs: 200                      # for all datasets
dcl_temperature: 0.1                      # 0.1 (ImageNet-1k), 0.07 (Cifar10, Cifar100, STL)
dcl_optimizer: "DCL"                 # SGD with Cosine annealing scheduler
dcl_base_lr: 0.03                         # 0.03 * batch_size/256
# backbone: "resnet50"                  # resnet18 (Cifar10, Cifar100, STL), resnet50 (ImageNet-1k, ImageNet-100)

################################ SWAV #######################################

#########################
#### data parameters ####
#########################
nmb_crops: [2]                    
size_crops: [224]
min_scale_crops: [0.14] #[0.2]
max_scale_crops: [1]

#########################
## swav specific params #
#########################
crops_for_assign: [0, 1]
swav_temperature: 0.1
epsilon: 0.05
sinkhorn_iterations: 3
feat_dim: 128
nmb_prototypes: 3000
queue_length: 0
epoch_queue_starts: 15

#########################
#### optim parameters ###
#########################
swav_batch_size: 64
swav_base_lr: 2.4 #4.8
final_lr: 0
swav_optimizer: "SwAV"
freeze_prototypes_niters: 313
warmup_epochs: 10
start_warmup: 0

#########################
#### dist parameters ###
#########################
world_size: -1
rank: 0
local_rank: 0

#########################
#### other parameters ###
#########################
hidden_mlp: 1024 #2048
workers: 4
checkpoint_freq: 25

################################ GENERAL ######################################
seed: 42                                      # sacred handles automatic seeding when passed in the config
base_epochs: 75 #600
base_image_size: 40
base_dataset: 5                               # dataset type. 0 for IMAGENET, 1 for CIFAR10, 2 for CHEST_XRAY, 3 for REAL
base_pretrain: True

do_gradual_base_pretrain: True

momentum: 0.9                                 # momentum of SGD solver
resume: ""                                    # path to latest checkpoint (default: none)
global_step: 0
current_epoch: 0
log_step: 1000

######################## target pretraining options
target_dataset: 5                             # dataset type. 0 for IMAGENET, 1 for CIFAR10, 2 for CHEST_XRAY, 3 for REAL
target_epochs: 200                            # this needs to be increased to about 800?? The experiment should start at 100 and then  vary it at an interval of 50
target_image_size: 80
target_lr: 1.0e-3                             # we expect the weights from the base pretraining to be good, so we don't want to distort them too quickly and too much.
target_pretrain: False 
target_epoch_num: 40
pretrain_path_loss_file: "pretrain_path_loss.pkl"


################################## LINEAR CLASSIFIER ###########################################
lc_dataset: 5                                 # dataset type. 0 for IMAGENET, 1 for CIFAR10, 2 for CHEST_XRAY, 3 for REAL
lc_image_size: 90                             # this depends on the dataset used

#########################
#### model parameters ###
#########################
global_pooling: True
use_bn: False

#########################
#### optim parameters ###
#########################
lc_optimizer: "Classifier" #"Adam-DCL"                # 
lc_epochs: 100
lc_batch_size: 32
lc_lr: 0.3 #1.0e-2 
nesterov: False
scheduler_type: "cosine"

# for multi-step learning rate decay
decay_epochs: [60, 80]
lc_gamma: 0.1

# for cosine learning rate schedule
lc_final_lr: 0

################################### AL ###########################################################
al_method: 0                                 # 0 for least confidence, 1 for entropy, 2 for both
al_finetune_trainer_epochs: 25
al_epochs: 20                                 # the default should be kept at 10, however due to compute limitations, I would use 3
al_batches: 5 #10                                # the default should be kept at 10, however due to compute limitations, I would use 20
al_finetune_batch_size: 256                   # I would like to keep the default at 32 or 64, but I made the current value 8 due to the cuda issue I am having. This seems to be the value that didn't produce an error
al_maintask_batch_size: 128                   # I would like to keep the default at 32 or 64, but I made the current value 8 due to the cuda issue I am having. This seems to be the value that didn't produce an error
al_trainer_sample_size: 1800 #5000                   # this specifies the amount of samples to be added to the training pool after each AL iteration
al_sample_percentage: 0.95                    # this specifies the percentage of the samples to be used for the target pretraining
al_gen_sample_percentage: 1
al_lr: 0.1
al_optimizer: "SGD-MultiStepV2"
al_weight_decay: 5.0e-4
do_al: False
al_path_loss_file: "al_path_loss.pkl"
al_pretext_from_pretrain: True                # this enables the pretext task to be finetuned using the weights of the pretrained model
al_train_maintask: False                      # this enables the training of the main task. If disabled, the generated pathloss from 'make_batch' is used for the second pretrain

ml_project: False                             # switches to ML Final Project mode 
do_al_for_ml_project: False                   # switches between AL and regular classification


################################### GAN ###########################################################
pretrained: "G_ema.pth"

lr_g_l: 0.0000001                             # lr for original linear layer in generator. default is 0.0000001
lr_g_batch_stat: 0.0005                       # lr for linear layer to generate scale and bias in generator
lr_embed: 0.05                                # lr for image embeddings
lr_bsa_l: 0.0005                              # lr for statistic (scale and bias) parameter for the original fc layer in generator. This is newly intoroduced learnable parameter for the pretrained GAN
lr_c_embed: 0.001                             # lr for class conditional embeddings
    
    
#loss settings
loss_per: 0.1                                 # scaling factor for perceptural loss. 
loss_emd: 0.1                                 # scaling factor for earth mover distance loss. 
loss_re: 0.02                                 # scaling factor for regularizer loss.
loss_norm_img: 1                              # normalize img loss or not
loss_norm_per: 1                              # normalize perceptural loss or not
loss_dist_per: "l2"                           # distance function for perceptual loss. l1 or l2

step: 3000                                    # Decrease lr by a factor of args.step_factor every <step> iterations
step_factor: 0.1                              # facter to multipy when decrease lr 

iters: 10000                                  # number of iterations.
gan_batch_size: 25                            # batch size
gan_epoch: 100
model_name: "biggan128-ada"                   # model. biggan128-ada

gen_images_path: "generated"                  # model checkpoint path
gan_resume: ""                                # path to resume training from
